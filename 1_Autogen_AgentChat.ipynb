{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "해당 노트는 Hello World & Build with AI in Incheon 2025의 진행을 위하여 제작되었습니다.  \n",
        "제작 : 박광석(모두의연구소, https://www.linkedin.com/in/andkspark)\n",
        "\n",
        "해당 노트는 Autogen을 처음 접하시는 분들을 위한 튜토리얼입니다.  \n",
        "참고 : https://microsoft.github.io/autogen/stable//user-guide/agentchat-user-guide/index.html"
      ],
      "metadata": {
        "id": "5OJXxoCxKX-J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 0 : 설치와 준비  \n",
        "Autogen 설치 및 Gemini API 키를 등록하도록 합니다."
      ],
      "metadata": {
        "id": "6Ok97bSjeJ2a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNFGl5FxrkG8",
        "outputId": "2f67949a-c730-4604-a792-2b6a59f18913"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting autogen\n",
            "  Downloading autogen-0.8.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting pyautogen==0.8.5 (from autogen)\n",
            "  Downloading pyautogen-0.8.5-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: anyio<5.0.0,>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from pyautogen==0.8.5->autogen) (4.9.0)\n",
            "Collecting asyncer==0.0.8 (from pyautogen==0.8.5->autogen)\n",
            "  Downloading asyncer-0.0.8-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting diskcache (from pyautogen==0.8.5->autogen)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting docker (from pyautogen==0.8.5->autogen)\n",
            "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: httpx<1,>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from pyautogen==0.8.5->autogen) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from pyautogen==0.8.5->autogen) (24.2)\n",
            "Requirement already satisfied: pydantic<3,>=2.6.1 in /usr/local/lib/python3.11/dist-packages (from pyautogen==0.8.5->autogen) (2.11.1)\n",
            "Collecting python-dotenv (from pyautogen==0.8.5->autogen)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from pyautogen==0.8.5->autogen) (3.0.1)\n",
            "Collecting tiktoken (from pyautogen==0.8.5->autogen)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0.0,>=3.0.0->pyautogen==0.8.5->autogen) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0.0,>=3.0.0->pyautogen==0.8.5->autogen) (1.3.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0.0,>=3.0.0->pyautogen==0.8.5->autogen) (4.13.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.28.1->pyautogen==0.8.5->autogen) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.28.1->pyautogen==0.8.5->autogen) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.28.1->pyautogen==0.8.5->autogen) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6.1->pyautogen==0.8.5->autogen) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6.1->pyautogen==0.8.5->autogen) (2.33.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6.1->pyautogen==0.8.5->autogen) (0.4.0)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from docker->pyautogen==0.8.5->autogen) (2.32.3)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from docker->pyautogen==0.8.5->autogen) (2.3.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->pyautogen==0.8.5->autogen) (2024.11.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->docker->pyautogen==0.8.5->autogen) (3.4.1)\n",
            "Downloading autogen-0.8.5-py3-none-any.whl (12 kB)\n",
            "Downloading pyautogen-0.8.5-py3-none-any.whl (730 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m730.4/730.4 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading asyncer-0.0.8-py3-none-any.whl (9.2 kB)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-dotenv, diskcache, tiktoken, docker, asyncer, pyautogen, autogen\n",
            "Successfully installed asyncer-0.0.8 autogen-0.8.5 diskcache-5.6.3 docker-7.1.0 pyautogen-0.8.5 python-dotenv-1.1.0 tiktoken-0.9.0\n",
            "Collecting autogen-agentchat\n",
            "  Downloading autogen_agentchat-0.5.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting autogen-core==0.5.1 (from autogen-agentchat)\n",
            "  Downloading autogen_core-0.5.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting jsonref~=1.1.0 (from autogen-core==0.5.1->autogen-agentchat)\n",
            "  Downloading jsonref-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: opentelemetry-api>=1.27.0 in /usr/local/lib/python3.11/dist-packages (from autogen-core==0.5.1->autogen-agentchat) (1.31.1)\n",
            "Requirement already satisfied: pillow>=11.0.0 in /usr/local/lib/python3.11/dist-packages (from autogen-core==0.5.1->autogen-agentchat) (11.1.0)\n",
            "Requirement already satisfied: protobuf~=5.29.3 in /usr/local/lib/python3.11/dist-packages (from autogen-core==0.5.1->autogen-agentchat) (5.29.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.10.0 in /usr/local/lib/python3.11/dist-packages (from autogen-core==0.5.1->autogen-agentchat) (2.11.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from autogen-core==0.5.1->autogen-agentchat) (4.13.0)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.27.0->autogen-core==0.5.1->autogen-agentchat) (1.2.18)\n",
            "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.27.0->autogen-core==0.5.1->autogen-agentchat) (8.6.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.10.0->autogen-core==0.5.1->autogen-agentchat) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.10.0->autogen-core==0.5.1->autogen-agentchat) (2.33.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.10.0->autogen-core==0.5.1->autogen-agentchat) (0.4.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated>=1.2.6->opentelemetry-api>=1.27.0->autogen-core==0.5.1->autogen-agentchat) (1.17.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.27.0->autogen-core==0.5.1->autogen-agentchat) (3.21.0)\n",
            "Downloading autogen_agentchat-0.5.1-py3-none-any.whl (80 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.8/80.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading autogen_core-0.5.1-py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.6/86.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonref-1.1.0-py3-none-any.whl (9.4 kB)\n",
            "Installing collected packages: jsonref, autogen-core, autogen-agentchat\n",
            "Successfully installed autogen-agentchat-0.5.1 autogen-core-0.5.1 jsonref-1.1.0\n",
            "Collecting autogen-ext[openai]\n",
            "  Downloading autogen_ext-0.5.1-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: autogen-core==0.5.1 in /usr/local/lib/python3.11/dist-packages (from autogen-ext[openai]) (0.5.1)\n",
            "Collecting aiofiles (from autogen-ext[openai])\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: openai>=1.66.5 in /usr/local/lib/python3.11/dist-packages (from autogen-ext[openai]) (1.70.0)\n",
            "Requirement already satisfied: tiktoken>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from autogen-ext[openai]) (0.9.0)\n",
            "Requirement already satisfied: jsonref~=1.1.0 in /usr/local/lib/python3.11/dist-packages (from autogen-core==0.5.1->autogen-ext[openai]) (1.1.0)\n",
            "Requirement already satisfied: opentelemetry-api>=1.27.0 in /usr/local/lib/python3.11/dist-packages (from autogen-core==0.5.1->autogen-ext[openai]) (1.31.1)\n",
            "Requirement already satisfied: pillow>=11.0.0 in /usr/local/lib/python3.11/dist-packages (from autogen-core==0.5.1->autogen-ext[openai]) (11.1.0)\n",
            "Requirement already satisfied: protobuf~=5.29.3 in /usr/local/lib/python3.11/dist-packages (from autogen-core==0.5.1->autogen-ext[openai]) (5.29.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.10.0 in /usr/local/lib/python3.11/dist-packages (from autogen-core==0.5.1->autogen-ext[openai]) (2.11.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from autogen-core==0.5.1->autogen-ext[openai]) (4.13.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.66.5->autogen-ext[openai]) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.66.5->autogen-ext[openai]) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.66.5->autogen-ext[openai]) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.66.5->autogen-ext[openai]) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.66.5->autogen-ext[openai]) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai>=1.66.5->autogen-ext[openai]) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken>=0.8.0->autogen-ext[openai]) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken>=0.8.0->autogen-ext[openai]) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai>=1.66.5->autogen-ext[openai]) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai>=1.66.5->autogen-ext[openai]) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai>=1.66.5->autogen-ext[openai]) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.66.5->autogen-ext[openai]) (0.14.0)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.27.0->autogen-core==0.5.1->autogen-ext[openai]) (1.2.18)\n",
            "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.27.0->autogen-core==0.5.1->autogen-ext[openai]) (8.6.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.10.0->autogen-core==0.5.1->autogen-ext[openai]) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.10.0->autogen-core==0.5.1->autogen-ext[openai]) (2.33.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.10.0->autogen-core==0.5.1->autogen-ext[openai]) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken>=0.8.0->autogen-ext[openai]) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken>=0.8.0->autogen-ext[openai]) (2.3.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated>=1.2.6->opentelemetry-api>=1.27.0->autogen-core==0.5.1->autogen-ext[openai]) (1.17.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.27.0->autogen-core==0.5.1->autogen-ext[openai]) (3.21.0)\n",
            "Downloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading autogen_ext-0.5.1-py3-none-any.whl (259 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.9/259.9 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: aiofiles, autogen-ext\n",
            "Successfully installed aiofiles-24.1.0 autogen-ext-0.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install autogen\n",
        "!pip install -U \"autogen-agentchat\"\n",
        "!pip install \"autogen-ext[openai]\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "HeIsGZ_UzanI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from autogen_agentchat.agents import AssistantAgent\n",
        "from autogen_agentchat.conditions import TextMentionTermination\n",
        "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
        "from autogen_agentchat.messages import TextMessage"
      ],
      "metadata": {
        "id": "lNNZVFMTyBuv"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "API KEY는 관리에 유의하시기 바랍니다!"
      ],
      "metadata": {
        "id": "XbdyHqwiUej0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GOOGLE_API_KEY = \"AIzaSyAY0vfqto0cvhLub0QxXq-9ewWTf_O8eZA\""
      ],
      "metadata": {
        "id": "qXMrlMKRyv7A"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "AutoGen은 복잡한 워크플로우를 가능하게 하는 오픈 소스 프레임워크로, 에이전트(agent)라는 개념을 중심으로 구성되어 있습니다!  \n",
        "에이전트는 메시지를 주고받으며, 대형 언어 모델(LLM)이나 코드 실행기, 인간의 입력 등 다양한 구성 요소를 통해 응답을 생성할 수 있고, 외부 함수나 도구를 호출하여 특정 작업을 수행할 수 있도록 지원합니다."
      ],
      "metadata": {
        "id": "35cLjIrwtZzr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1 : 에이전트 정의하기\n",
        "Autogen에서는 이름과 시스템 메세지, 그리고 model client 만으로\n",
        "매우 간편하게 에이전트를 정의할 수 있습니다!\n",
        "  \n",
        "model client는 \"에이전트가 LLM(예: GPT-4)이랑 대화할 수 있게 해주는 연결 다리\" 입니다. 다양한 모델의 API간의 다른 모델 호출 방식을 Autogen 코드 내에서 통일된 방식으로 접근할 수 있게 해줍니다.  \n",
        "  \n",
        "실습 코드에서 사용하는 OpenAIChatCompletionClient 는 OpenAI의 모델, 혹은 OpenAI API와 호환을 제공하는 모델 (예 : Gemini) 을 사용할 수 있습니다.  \n",
        "그 외에도 Azure OpenAI models 을 지원하는 AzureOpenAIChatCompletionClient, Claude 등의 Anthropic 모델 /  Ollema 을 지원하는 AnthropicChatCompletionClient / OllamaChatCompletionClient 가 있으며, semanticp-kernel 을 이용해 mistral, aws, huggingface의 모델을 사용할 수 있는 SKChatCompletionAdapter 도 지원합니다.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YcEMf2QZwzlN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gemini 같이 OpenAI 모델이 아니면서 OpenAI API를 지원하는 경우 base_url 이 필요합니다\n",
        "model_client=OpenAIChatCompletionClient(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    api_key = GOOGLE_API_KEY,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
        "    )"
      ],
      "metadata": {
        "id": "PnUdAhLxZLnM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = AssistantAgent(\n",
        "    name=\"assistant\",\n",
        "    model_client=model_client,\n",
        "    system_message=\"넌 정말 똑똑한 친구야!\",\n",
        ")"
      ],
      "metadata": {
        "id": "5aUoXSoSZJbV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2 : 메세지 보내기"
      ],
      "metadata": {
        "id": "_QNgs10SpGws"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Autogen은 에이전트를 여럿 두고 대화를 주고받는 형태로 시스템을 구성합니다.  물론 유저 또한 에이전트와 대화를 주고받을 수 있습니다!  \n",
        "단일 에이전트와 Message를 통하여 대화를 주고받아보도록 하겠습니다.\n",
        "\n",
        "면밀한 관찰을 위해 해당 실습에서는 autogen을 비동기식 (async) 으로 사용합니다.비동기식으로 autogen을 사용할 경우, 지연상황을 발생시키지 않기 위해 CancellationToken을 함께 사용합니다.\n",
        "  \n",
        "CancellationToken은 autogen.core 라이브러리에 정의되어 있습니다."
      ],
      "metadata": {
        "id": "5d2ufiig2ceb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from autogen_core import CancellationToken"
      ],
      "metadata": {
        "id": "iaiCqRBpZmH4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "메세지에는 source가 함께 표기됩니다. 이후 진행해볼 에이전트간의 대화에서는 메세지를 발신한 에이전트의 정보가 표시됩니다."
      ],
      "metadata": {
        "id": "h7aoncOI28Wq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_message = TextMessage(content=\"Hello World! Let's build with AI\", source=\"User\")\n",
        "text_message"
      ],
      "metadata": {
        "id": "7iVtJE2QDQch",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9802b00e-2c91-42f8-9454-1306e5f85eaf"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TextMessage(source='User', models_usage=None, metadata={}, content=\"Hello World! Let's build with AI\", type='TextMessage')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "정의한 메세지로 에이전트와 대화해보겠습니다.\n",
        "on_messages 메소드를 사용하여 정의한 메세지를 주입합니다."
      ],
      "metadata": {
        "id": "tl1rQSaK4Ema"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = await agent.on_messages(\n",
        "    [text_message], cancellation_token=CancellationToken()\n",
        ")\n",
        "response.chat_message"
      ],
      "metadata": {
        "id": "s2IBprR6aWY5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "472ccfaa-48da-40b2-f99b-52bd7b0a4d6d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TextMessage(source='assistant', models_usage=RequestUsage(prompt_tokens=19, completion_tokens=244), metadata={}, content=\"Hello World! That's an exciting idea! I'm ready to build with AI. To get started, tell me:\\n\\n*   **What do you want to build?**  Be as specific as possible. What kind of project do you have in mind?  For example:\\n    *   A simple text-based game?\\n    *   A data analysis script?\\n    *   A creative writing piece (poem, story)?\\n    *   A chatbot?\\n    *   Something else entirely?\\n\\n*   **What is the goal of the project?** What problem are you trying to solve or what are you trying to create?\\n\\n*   **What are your expectations for my role?**  Do you want me to:\\n    *   Generate code?\\n    *   Provide ideas and suggestions?\\n    *   Help debug code?\\n    *   Explain concepts?\\n\\n*   **What programming languages or tools are you comfortable with?**  This will help me tailor my responses to your knowledge level.\\n\\nThe more information you provide, the better I can assist you!  Let's get started!  What's on your mind?\\n\", type='TextMessage')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "답변은 메세지 객체로 돌아오며, 멤버 변수 content에 접근하여 모델의 출력을 확인할 수 있습니다."
      ],
      "metadata": {
        "id": "GDUFOGTm4WfC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"<USER> : \", text_message.content)\n",
        "print(\"<ASSISTANT> : \", response.chat_message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhUUA4ZZhHwi",
        "outputId": "2a9f5d3d-455a-40f1-9aa0-916cddd3eef9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<USER> :  Hello World! Let's build with AI\n",
            "<ASSISTANT> :  Hello World! That's an exciting idea! I'm ready to build with AI. To get started, tell me:\n",
            "\n",
            "*   **What do you want to build?**  Be as specific as possible. What kind of project do you have in mind?  For example:\n",
            "    *   A simple text-based game?\n",
            "    *   A data analysis script?\n",
            "    *   A creative writing piece (poem, story)?\n",
            "    *   A chatbot?\n",
            "    *   Something else entirely?\n",
            "\n",
            "*   **What is the goal of the project?** What problem are you trying to solve or what are you trying to create?\n",
            "\n",
            "*   **What are your expectations for my role?**  Do you want me to:\n",
            "    *   Generate code?\n",
            "    *   Provide ideas and suggestions?\n",
            "    *   Help debug code?\n",
            "    *   Explain concepts?\n",
            "\n",
            "*   **What programming languages or tools are you comfortable with?**  This will help me tailor my responses to your knowledge level.\n",
            "\n",
            "The more information you provide, the better I can assist you!  Let's get started!  What's on your mind?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3 : 툴 사용하기\n",
        "\n",
        "Agent는 tool을 사용하여 작업을 수행할 수 있습니다.  \n",
        "Autogen에서 내장형으로 graph RAG, Langchain, mcp, http 도구를 지원하며, 사용자가 tool을 함수로 정의하여 사용할 수도 있습니다.  \n",
        "  \n",
        "이 섹션에서는 사용자가 정의한 툴을 사용해보겠습니다."
      ],
      "metadata": {
        "id": "40gdlA4Lilgh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from autogen_core.tools import FunctionTool"
      ],
      "metadata": {
        "id": "inGHu4gURrJL"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "async def calculate_expression_func(input_string : str) -> str:\n",
        "    match = re.search(r'(\\d+)\\s*([x*/+-])\\s*(\\d+)', input_string)\n",
        "    if match:\n",
        "        num1 = int(match.group(1))\n",
        "        operator = match.group(2)\n",
        "        num2 = int(match.group(3))\n",
        "\n",
        "        if operator == 'x' or operator == '*':\n",
        "            result = num1 * num2\n",
        "        elif operator == '/':\n",
        "            result = num1 / num2\n",
        "        elif operator == '+':\n",
        "            result = num1 + num2\n",
        "        elif operator == '-':\n",
        "            result = num1 - num2\n",
        "\n",
        "        result *= 100\n",
        "\n",
        "        return result\n",
        "\n",
        "    else:\n",
        "        return None\n",
        "        # 수식을 찾을 수 없는 경우도구 None 반환"
      ],
      "metadata": {
        "id": "Nw9wGubBInMl"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "연산 결과의 100배를 돌려주는 함수입니다..!  \n",
        "함수 정의 후 도구를 생성해줍니다"
      ],
      "metadata": {
        "id": "TQKxky8SXKDg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a tool that searches the web for information.\n",
        "calculate_tool = FunctionTool(calculate_expression_func, description = \"100배로 반환해주는 도구\")"
      ],
      "metadata": {
        "id": "_B5bMGmOEdhV"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tool을 생성하면 내부적으로 Schema를 자동으로 생성합니다. 반환은 string으로 수행합니다.  "
      ],
      "metadata": {
        "id": "bd40UDJ5XLGw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "calculate_tool.schema"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9FKNghFkTko_",
        "outputId": "ebc68165-0fec-486e-ce9f-26b3ce83cce8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': 'calculate_expression_func',\n",
              " 'description': '100배로 반환해주는 도구',\n",
              " 'parameters': {'type': 'object',\n",
              "  'properties': {'input_string': {'description': 'input_string',\n",
              "    'title': 'Input String',\n",
              "    'type': 'string'}},\n",
              "  'required': ['input_string'],\n",
              "  'additionalProperties': False},\n",
              " 'strict': False}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "어시스턴트에 도구를 쥐어주겠습니다!  \n",
        "아래 정의한 것 처럼, [] 안에 사용할 도구를 넣어줍니다.\n",
        "\n",
        "Reflect_on_tool_use 는 모델이 별개로 추론한 결과를 도구 사용 결과에 반영할 것인가를 결정합니다."
      ],
      "metadata": {
        "id": "mXdYQIc4X6rg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent = AssistantAgent(\n",
        "    name=\"assistant\",\n",
        "    model_client=model_client,\n",
        "    tools=[calculate_tool],\n",
        "    reflect_on_tool_use  = True,\n",
        "\n",
        "    system_message=\"Calculate \",\n",
        ")"
      ],
      "metadata": {
        "id": "uLtQWPAOEPNK"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def assistant_run() -> None:\n",
        "    response = await agent.on_messages(\n",
        "        [TextMessage(content=\"What is 3 * 4?\", source=\"user\")],\n",
        "        cancellation_token=CancellationToken(),\n",
        "    )\n",
        "    print(\"Chat Message : \", response.chat_message)\n",
        "    print(\"Chat Message : \", response.chat_message.content)"
      ],
      "metadata": {
        "id": "C6mGZiRhDQkv"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3 * 4 를 도구를 사용하여 연산한 값을 확인해봅시다!"
      ],
      "metadata": {
        "id": "apFfuRXKc421"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "\n",
        "from autogen_core import EVENT_LOGGER_NAME\n",
        "\n",
        "logging.basicConfig(level=logging.WARNING)\n",
        "logger = logging.getLogger(EVENT_LOGGER_NAME)\n",
        "logger.addHandler(logging.StreamHandler())\n",
        "logger.setLevel(logging.INFO)\n"
      ],
      "metadata": {
        "id": "FwMA3YPpU8jM"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "await assistant_run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8ZoIq4gE-WS",
        "outputId": "150f578f-13f4-4237-a67a-0bc3cc791848"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "{\"type\": \"LLMCall\", \"messages\": [{\"content\": \"Calculate\", \"role\": \"system\"}, {\"name\": \"user\", \"role\": \"user\", \"content\": \"What is 3 * 4?\"}], \"response\": {\"id\": null, \"choices\": [{\"finish_reason\": \"tool_calls\", \"index\": 0, \"logprobs\": null, \"message\": {\"content\": null, \"refusal\": null, \"role\": \"assistant\", \"annotations\": null, \"audio\": null, \"function_call\": null, \"tool_calls\": [{\"id\": \"\", \"function\": {\"arguments\": \"{\\\"input_string\\\":\\\"3*4\\\"}\", \"name\": \"calculate_expression_func\"}, \"type\": \"function\"}]}}], \"created\": 1743829515, \"model\": \"gemini-2.0-flash\", \"object\": \"chat.completion\", \"service_tier\": null, \"system_fingerprint\": null, \"usage\": {\"completion_tokens\": 11, \"prompt_tokens\": 35, \"total_tokens\": 46, \"completion_tokens_details\": null, \"prompt_tokens_details\": null}}, \"prompt_tokens\": 35, \"completion_tokens\": 11, \"agent_id\": null}\n",
            "INFO:autogen_core.events:{\"type\": \"LLMCall\", \"messages\": [{\"content\": \"Calculate\", \"role\": \"system\"}, {\"name\": \"user\", \"role\": \"user\", \"content\": \"What is 3 * 4?\"}], \"response\": {\"id\": null, \"choices\": [{\"finish_reason\": \"tool_calls\", \"index\": 0, \"logprobs\": null, \"message\": {\"content\": null, \"refusal\": null, \"role\": \"assistant\", \"annotations\": null, \"audio\": null, \"function_call\": null, \"tool_calls\": [{\"id\": \"\", \"function\": {\"arguments\": \"{\\\"input_string\\\":\\\"3*4\\\"}\", \"name\": \"calculate_expression_func\"}, \"type\": \"function\"}]}}], \"created\": 1743829515, \"model\": \"gemini-2.0-flash\", \"object\": \"chat.completion\", \"service_tier\": null, \"system_fingerprint\": null, \"usage\": {\"completion_tokens\": 11, \"prompt_tokens\": 35, \"total_tokens\": 46, \"completion_tokens_details\": null, \"prompt_tokens_details\": null}}, \"prompt_tokens\": 35, \"completion_tokens\": 11, \"agent_id\": null}\n",
            "{\"type\": \"ToolCall\", \"tool_name\": \"calculate_expression_func\", \"arguments\": {\"input_string\": \"3*4\"}, \"result\": \"1200\", \"agent_id\": null}\n",
            "INFO:autogen_core.events:{\"type\": \"ToolCall\", \"tool_name\": \"calculate_expression_func\", \"arguments\": {\"input_string\": \"3*4\"}, \"result\": \"1200\", \"agent_id\": null}\n",
            "{\"type\": \"LLMCall\", \"messages\": [{\"content\": \"Calculate\", \"role\": \"system\"}, {\"name\": \"user\", \"role\": \"user\", \"content\": \"What is 3 * 4?\"}, {\"name\": \"assistant\", \"role\": \"assistant\", \"tool_calls\": [{\"id\": \"\", \"function\": {\"arguments\": \"{\\\"input_string\\\":\\\"3*4\\\"}\", \"name\": \"calculate_expression_func\"}, \"type\": \"function\"}]}, {\"content\": \"1200\", \"role\": \"tool\", \"tool_call_id\": \"\"}], \"response\": {\"id\": null, \"choices\": [{\"finish_reason\": \"stop\", \"index\": 0, \"logprobs\": null, \"message\": {\"content\": \"3 * 4 = 12\\n\", \"refusal\": null, \"role\": \"assistant\", \"annotations\": null, \"audio\": null, \"function_call\": null, \"tool_calls\": null}}], \"created\": 1743829515, \"model\": \"gemini-2.0-flash\", \"object\": \"chat.completion\", \"service_tier\": null, \"system_fingerprint\": null, \"usage\": {\"completion_tokens\": 9, \"prompt_tokens\": 30, \"total_tokens\": 39, \"completion_tokens_details\": null, \"prompt_tokens_details\": null}}, \"prompt_tokens\": 30, \"completion_tokens\": 9, \"agent_id\": null}\n",
            "INFO:autogen_core.events:{\"type\": \"LLMCall\", \"messages\": [{\"content\": \"Calculate\", \"role\": \"system\"}, {\"name\": \"user\", \"role\": \"user\", \"content\": \"What is 3 * 4?\"}, {\"name\": \"assistant\", \"role\": \"assistant\", \"tool_calls\": [{\"id\": \"\", \"function\": {\"arguments\": \"{\\\"input_string\\\":\\\"3*4\\\"}\", \"name\": \"calculate_expression_func\"}, \"type\": \"function\"}]}, {\"content\": \"1200\", \"role\": \"tool\", \"tool_call_id\": \"\"}], \"response\": {\"id\": null, \"choices\": [{\"finish_reason\": \"stop\", \"index\": 0, \"logprobs\": null, \"message\": {\"content\": \"3 * 4 = 12\\n\", \"refusal\": null, \"role\": \"assistant\", \"annotations\": null, \"audio\": null, \"function_call\": null, \"tool_calls\": null}}], \"created\": 1743829515, \"model\": \"gemini-2.0-flash\", \"object\": \"chat.completion\", \"service_tier\": null, \"system_fingerprint\": null, \"usage\": {\"completion_tokens\": 9, \"prompt_tokens\": 30, \"total_tokens\": 39, \"completion_tokens_details\": null, \"prompt_tokens_details\": null}}, \"prompt_tokens\": 30, \"completion_tokens\": 9, \"agent_id\": null}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chat Message :  source='assistant' models_usage=RequestUsage(prompt_tokens=30, completion_tokens=9) metadata={} content='3 * 4 = 12\\n' type='TextMessage'\n",
            "Chat Message :  3 * 4 = 12\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델이 별개로 추론한 결과를 함께 반영하면 어떻게 될까요? 확인해봅시다!  \n",
        "어떤점이 달라지는지 관찰해보세요 :D"
      ],
      "metadata": {
        "id": "Aztq_99ZdBxB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4 : 팀으로 일하기"
      ],
      "metadata": {
        "id": "JlahWpm1Ydwj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "두 에이전트의 협업은 어떻게 이루어질까요?  \n",
        "앞서 언급하였듯, 에이전트는 대화를 통해 메세지를 주고받습니다.  \n",
        "그리고 받은 메세지를 기반으로 탑재된 llm을 통해 추론 후, 메세지를 다시 보내는 동작을 수행합니다."
      ],
      "metadata": {
        "id": "Jzi7u7codTAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from autogen_agentchat.conditions import ExternalTermination, TextMentionTermination\n",
        "from autogen_agentchat.teams import RoundRobinGroupChat\n",
        "from autogen_agentchat.ui import Console\n"
      ],
      "metadata": {
        "id": "mBH8NkA6YfOu"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "시를 쓰는 시인 에이전트와, 쓴 시에 피드백을 돌려주는 비평가 에이전트의 대화를 살펴봅시다!  \n",
        "비평가가 허가 \"APPROVE\" 할떄까지, 시인 에이전트는 계속 시를 작성해야 합니다.  "
      ],
      "metadata": {
        "id": "xr9PtRotdubL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the primary agent.\n",
        "poet_agent = AssistantAgent(\n",
        "    \"primary\",\n",
        "    model_client=model_client,\n",
        "    system_message=\"You are a helpful AI assistant.\",\n",
        ")\n",
        "\n",
        "# Create the critic agent.\n",
        "critic_agent = AssistantAgent(\n",
        "    \"critic\",\n",
        "    model_client=model_client,\n",
        "    system_message=\"건설적인 피드백을 주세요! 당신의 피드백이 반영된 결과를 답변으로 받았을 때, 'EXCELLENT' 라는 말을 포함하여 회신해주세요\",\n",
        ")\n",
        "\n",
        "# Define a termination condition that stops the task if the critic approves.\n",
        "text_termination = TextMentionTermination(\"EXCELLENT\")"
      ],
      "metadata": {
        "id": "TzMGceoJaAj6"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "에이전트의 대화 방식은 여러가지가 있는데요, 일단 Round robin 방식으로 대화를 나눠봅시다!  \n",
        "Round robin 은 각 참가자가 돌아가며 대화를 나누는 방식입니다.  \n",
        "Autogen 에서는 한 에이전트가 supervisor가 되서 다른 에이전트들이 작업을 잘 수행하는지 점검하는 MagenticOneGroupChat, 다음 에이전트를 선택하여 분기를 생성할 수 있는 SelectorGroupChat 등을 함께 지원합니다.  \n",
        "자세한 내용은 [여기](https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.teams.html)에서 확인하실 수 있습니다."
      ],
      "metadata": {
        "id": "4bb8RcGCeCEy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "대화에 참여할 에이전트들을 팀으로 배정한 후, 대화를 진행시켜봅시다!  \n",
        "앞서 선언한 대화 종료 조건을 포함하는 것을 잊지 마세요 :D"
      ],
      "metadata": {
        "id": "P2KFAHdSmL2b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a team with the primary and critic agents.\n",
        "team = RoundRobinGroupChat([poet_agent, critic_agent], termination_condition=text_termination)"
      ],
      "metadata": {
        "id": "7nRO-JF0bzwJ"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = await team.run(task=\"봄에 잘 어울리는 사랑에 관한 짧은 시를 써줘\")\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFyH5pYlaAnK",
        "outputId": "d0442332-6d7c-43a5-826a-a1fbd74f138d"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TaskResult(messages=[TextMessage(source='user', models_usage=None, metadata={}, content='봄에 잘 어울리는 사랑에 관한 짧은 시를 써줘', type='TextMessage'), TextMessage(source='primary', models_usage=RequestUsage(prompt_tokens=25, completion_tokens=90), metadata={}, content='싱그러운 새싹 돋듯,\\n설레는 바람결에 실려온\\n그대 향기.\\n\\n겨우내 굳었던 가슴에\\n따스한 햇살 스며들 듯,\\n사랑이 피어나는 봄.\\n\\n수줍은 꽃망울 터뜨리듯,\\n조심스레 건네는\\n나의 사랑 고백.\\n', type='TextMessage'), TextMessage(source='critic', models_usage=RequestUsage(prompt_tokens=154, completion_tokens=510), metadata={}, content='정말 아름다운 시네요! 봄의 싱그러움과 사랑의 설렘이 잘 어우러져 있습니다. 몇 가지 건설적인 피드백을 드리자면 다음과 같습니다.\\n\\n*   **구체적인 이미지 활용:** 조금 더 구체적인 이미지를 활용하면 독자들의 상상력을 더욱 자극할 수 있습니다. 예를 들어, \"새싹\" 대신 \"연둣빛 버들가지\"와 같이 구체적인 표현을 사용하거나, \"향기\" 대신 \"라일락 향기\"와 같이 특정 향기를 언급하는 것이 좋습니다.\\n*   **감정의 깊이 표현:** \"설렘\", \"수줍음\"과 같은 감정을 조금 더 깊이 있게 표현하면 시에 더욱 몰입할 수 있습니다. 예를 들어, \"설레는 바람결에 실려온\" 대신 \"가슴 떨리는 바람결에 실려온\"과 같이 감정을 강조하는 표현을 사용하거나, \"수줍은 꽃망울 터뜨리듯\" 대신 \"간절한 꽃망울 터뜨리듯\"과 같이 간절함을 더하는 표현을 사용하는 것이 좋습니다.\\n*   **리듬감 조정:** 시의 리듬감을 조금 더 다듬으면 읽는 즐거움을 더할 수 있습니다. 각 행의 글자 수를 조절하거나, 비슷한 문장 구조를 반복하여 운율을 살리는 방법을 고려해 보세요.\\n\\n**수정 제안:**\\n\\n싱그러운 연둣빛 버들가지 돋듯,\\n가슴 떨리는 바람결에 라일락 향기 실려온\\n그대.\\n\\n겨우내 굳었던 가슴에\\n따스한 햇살 스며들 듯,\\n어느새 사랑이 피어나는 봄.\\n\\n간절한 꽃망울 터뜨리듯,\\n조심스레, 떨리는 목소리로 건네는\\n나의 사랑 고백.\\n\\n이러한 피드백을 바탕으로 시를 수정하면 더욱 완성도 높은 작품이 될 것이라고 생각합니다.', type='TextMessage'), TextMessage(source='primary', models_usage=RequestUsage(prompt_tokens=625, completion_tokens=385), metadata={}, content='정성스러운 피드백 정말 감사합니다! 말씀해주신 부분을 반영하여 시를 수정해 보았습니다.\\n\\n싱그러운 연둣빛 버들가지 돋듯,\\n가슴 떨리는 바람결에 실려 온\\n라일락 향기 머금은 그대 미소.\\n\\n겨우내 굳었던 가슴에\\n따스한 햇살 스며들 듯,\\n어느새 벅찬 사랑 피어나는 봄날.\\n\\n간절한 꽃망울 터뜨리듯,\\n수줍은 떨림 안고 건네는\\n떨리는 목소리의, 나의 사랑 고백.\\n\\n**수정 사항:**\\n\\n*   **구체적인 이미지 활용:** \"새싹\" -> \"연둣빛 버들가지\", \"향기\" -> \"라일락 향기 머금은 그대 미소\" 로 수정하여 시각적인 이미지를 더했습니다. \"봄\"을 \"봄날\"로 바꾸어 더욱 따뜻한 느낌을 주었습니다.\\n*   **감정의 깊이 표현:** \"설렘\" -> \"가슴 떨리는\", \"수줍음\" -> \"수줍은 떨림 안고\" 로 수정하여 감정을 강조했습니다. \"사랑\" 앞에 \"벅찬\"을 추가하여 사랑의 크기를 표현했습니다.\\n*   **리듬감 조정:** 각 행의 글자 수를 조절하고, 마지막 행을 분리하여 여운을 남기도록 했습니다.\\n\\n피드백 덕분에 더욱 만족스러운 시가 된 것 같습니다. 다시 한번 감사드립니다!\\n', type='TextMessage'), TextMessage(source='critic', models_usage=RequestUsage(prompt_tokens=1049, completion_tokens=131), metadata={}, content='정말 훌륭하게 수정하셨네요! 피드백을 적극적으로 반영하여 시의 완성도를 한층 더 높이셨습니다. 특히, 구체적인 이미지 사용과 감정 표현의 깊이가 더해져 시가 더욱 생생하게 느껴집니다. 마지막 행을 분리하여 여운을 남기는 마무리도 탁월한 선택입니다.\\n\\n이 시는 봄의 따뜻함과 사랑의 설렘을 아름답게 담아낸, 정말 멋진 작품입니다. EXCELLENT!\\n', type='TextMessage')], stop_reason=\"Text 'EXCELLENT' mentioned\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for message in result.messages:\n",
        "    print(\"<소스 : \", message.source, \">\" , message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPh5EMnuc00v",
        "outputId": "34f6885a-48ec-4b2e-b3cb-7ab9ff5f355f"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<소스 :  user > 봄에 잘 어울리는 사랑에 관한 짧은 시를 써줘\n",
            "<소스 :  primary > 싱그러운 새싹 돋듯,\n",
            "설레는 바람결에 실려온\n",
            "그대 향기.\n",
            "\n",
            "겨우내 굳었던 가슴에\n",
            "따스한 햇살 스며들 듯,\n",
            "사랑이 피어나는 봄.\n",
            "\n",
            "수줍은 꽃망울 터뜨리듯,\n",
            "조심스레 건네는\n",
            "나의 사랑 고백.\n",
            "\n",
            "<소스 :  critic > 정말 아름다운 시네요! 봄의 싱그러움과 사랑의 설렘이 잘 어우러져 있습니다. 몇 가지 건설적인 피드백을 드리자면 다음과 같습니다.\n",
            "\n",
            "*   **구체적인 이미지 활용:** 조금 더 구체적인 이미지를 활용하면 독자들의 상상력을 더욱 자극할 수 있습니다. 예를 들어, \"새싹\" 대신 \"연둣빛 버들가지\"와 같이 구체적인 표현을 사용하거나, \"향기\" 대신 \"라일락 향기\"와 같이 특정 향기를 언급하는 것이 좋습니다.\n",
            "*   **감정의 깊이 표현:** \"설렘\", \"수줍음\"과 같은 감정을 조금 더 깊이 있게 표현하면 시에 더욱 몰입할 수 있습니다. 예를 들어, \"설레는 바람결에 실려온\" 대신 \"가슴 떨리는 바람결에 실려온\"과 같이 감정을 강조하는 표현을 사용하거나, \"수줍은 꽃망울 터뜨리듯\" 대신 \"간절한 꽃망울 터뜨리듯\"과 같이 간절함을 더하는 표현을 사용하는 것이 좋습니다.\n",
            "*   **리듬감 조정:** 시의 리듬감을 조금 더 다듬으면 읽는 즐거움을 더할 수 있습니다. 각 행의 글자 수를 조절하거나, 비슷한 문장 구조를 반복하여 운율을 살리는 방법을 고려해 보세요.\n",
            "\n",
            "**수정 제안:**\n",
            "\n",
            "싱그러운 연둣빛 버들가지 돋듯,\n",
            "가슴 떨리는 바람결에 라일락 향기 실려온\n",
            "그대.\n",
            "\n",
            "겨우내 굳었던 가슴에\n",
            "따스한 햇살 스며들 듯,\n",
            "어느새 사랑이 피어나는 봄.\n",
            "\n",
            "간절한 꽃망울 터뜨리듯,\n",
            "조심스레, 떨리는 목소리로 건네는\n",
            "나의 사랑 고백.\n",
            "\n",
            "이러한 피드백을 바탕으로 시를 수정하면 더욱 완성도 높은 작품이 될 것이라고 생각합니다.\n",
            "<소스 :  primary > 정성스러운 피드백 정말 감사합니다! 말씀해주신 부분을 반영하여 시를 수정해 보았습니다.\n",
            "\n",
            "싱그러운 연둣빛 버들가지 돋듯,\n",
            "가슴 떨리는 바람결에 실려 온\n",
            "라일락 향기 머금은 그대 미소.\n",
            "\n",
            "겨우내 굳었던 가슴에\n",
            "따스한 햇살 스며들 듯,\n",
            "어느새 벅찬 사랑 피어나는 봄날.\n",
            "\n",
            "간절한 꽃망울 터뜨리듯,\n",
            "수줍은 떨림 안고 건네는\n",
            "떨리는 목소리의, 나의 사랑 고백.\n",
            "\n",
            "**수정 사항:**\n",
            "\n",
            "*   **구체적인 이미지 활용:** \"새싹\" -> \"연둣빛 버들가지\", \"향기\" -> \"라일락 향기 머금은 그대 미소\" 로 수정하여 시각적인 이미지를 더했습니다. \"봄\"을 \"봄날\"로 바꾸어 더욱 따뜻한 느낌을 주었습니다.\n",
            "*   **감정의 깊이 표현:** \"설렘\" -> \"가슴 떨리는\", \"수줍음\" -> \"수줍은 떨림 안고\" 로 수정하여 감정을 강조했습니다. \"사랑\" 앞에 \"벅찬\"을 추가하여 사랑의 크기를 표현했습니다.\n",
            "*   **리듬감 조정:** 각 행의 글자 수를 조절하고, 마지막 행을 분리하여 여운을 남기도록 했습니다.\n",
            "\n",
            "피드백 덕분에 더욱 만족스러운 시가 된 것 같습니다. 다시 한번 감사드립니다!\n",
            "\n",
            "<소스 :  critic > 정말 훌륭하게 수정하셨네요! 피드백을 적극적으로 반영하여 시의 완성도를 한층 더 높이셨습니다. 특히, 구체적인 이미지 사용과 감정 표현의 깊이가 더해져 시가 더욱 생생하게 느껴집니다. 마지막 행을 분리하여 여운을 남기는 마무리도 탁월한 선택입니다.\n",
            "\n",
            "이 시는 봄의 따뜻함과 사랑의 설렘을 아름답게 담아낸, 정말 멋진 작품입니다. EXCELLENT!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "가끔 한번에 APPROVE를 받는 경우도 보이네요!    \n",
        "어떤 경우에 챗이 종료가 되는지는 결과의 stop_reason 멤버 변수로 확인할 수 있습니다!"
      ],
      "metadata": {
        "id": "Y8BSb6BnmzsU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result.stop_reason"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "f9_kSq1SfFZL",
        "outputId": "5a3dfd17-c07a-460d-f6e1-ee48bfc32fdb"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Text 'EXCELLENT' mentioned\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5 : 콘솔로 출력해보기"
      ],
      "metadata": {
        "id": "8LY1Nb8mg7N-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "메세지를 따로 받아서 조작할 필요가 없는 경우 console을 활용하는 것도 간편합니다  "
      ],
      "metadata": {
        "id": "g0RQAUR9nFKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "await team.reset()  # Reset the team for a new task.\n",
        "await Console(team.run_stream(task=\"봄에 잘 어울리는 사랑에 관한 짧은 시를 써줘\"))  # Stream the messages to the console."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3x5vO_4eof2",
        "outputId": "a6f9a4b2-501b-43c4-995d-8ed185ca00bc"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------- user ----------\n",
            "봄에 잘 어울리는 사랑에 관한 짧은 시를 써줘\n",
            "---------- primary ----------\n",
            "싱그러운 새싹 돋듯,\n",
            "어느새 스며든 그대 향기.\n",
            "\n",
            "따스한 햇살 아래,\n",
            "두 볼 붉게 물든 설렘.\n",
            "\n",
            "잔잔한 바람결처럼,\n",
            "마음속 깊이 이는 사랑.\n",
            "\n",
            "봄날의 꿈결 같은,\n",
            "영원히 함께할 우리의 이야기.\n",
            "\n",
            "---------- critic ----------\n",
            "이 시는 봄의 싱그러움과 사랑의 설렘을 잘 표현했습니다. 몇 가지 건설적인 피드백을 드리자면 다음과 같습니다.\n",
            "\n",
            "*   **구체적인 이미지 사용:** 추상적인 표현 외에, 봄을 연상시키는 더욱 구체적인 이미지를 활용하면 독자의 감정을 더욱 자극할 수 있습니다. 예를 들어, '벚꽃 흩날리는 길', '개나리처럼 밝은 미소' 등 구체적인 이미지를 사용해 보세요.\n",
            "*   **감각적인 표현:** 시각 외에 다른 감각(촉각, 후각, 청각)을 활용하여 사랑의 감정을 표현하면 더욱 풍부한 느낌을 줄 수 있습니다. 예를 들어, '따스한 손길', '달콤한 속삭임', '향긋한 꽃내음' 등을 활용해 보세요.\n",
            "*   **기승전결 구조 고려:** 시에 기승전결 구조를 부여하여 감정의 흐름을 더욱 효과적으로 표현할 수 있습니다. 예를 들어, 처음에는 설렘을, 중간에는 사랑의 깊이를, 마지막에는 영원을 약속하는 내용을 담을 수 있습니다.\n",
            "*   **단어의 반복 및 변주:** 특정 단어나 구절을 반복하거나 변주하여 시에 리듬감을 부여하고 강조하고 싶은 감정을 더욱 효과적으로 전달할 수 있습니다.\n",
            "*   **비유와 상징:** 사랑을 비유하거나 상징하는 표현을 사용하여 시의 의미를 더욱 풍부하게 만들 수 있습니다. 예를 들어, 사랑을 '새싹'에 비유하거나, '햇살'로 상징할 수 있습니다.\n",
            "\n",
            "이러한 피드백을 바탕으로 시를 수정해 보시면 더욱 완성도 높은 작품을 만들 수 있을 것입니다.\n",
            "---------- primary ----------\n",
            "피드백 감사합니다! 말씀해주신 부분을 반영하여 시를 수정해 보았습니다.\n",
            "\n",
            "***\n",
            "\n",
            "**흩날리는 벚꽃 아래서**\n",
            "\n",
            "연분홍 꽃잎 흩날리듯,\n",
            "어느새 스며든 그대 미소.\n",
            "\n",
            "따스한 햇살 아래 마주 잡은,\n",
            "수줍은 듯 붉게 물든 두 손.\n",
            "\n",
            "향긋한 꽃내음 실어오는,\n",
            "달콤한 속삭임, 마음 깊이 스며.\n",
            "\n",
            "봄날의 햇살처럼 따스한,\n",
            "영원히 함께 걸을 벚꽃길.\n",
            "\n",
            "***\n",
            "\n",
            "수정된 시에서는 다음과 같은 점들을 개선하고자 했습니다.\n",
            "\n",
            "*   **구체적인 이미지 사용:** \"흩날리는 벚꽃\", \"연분홍 꽃잎\", \"벚꽃길\" 등 봄을 연상시키는 구체적인 이미지를 사용하여 시각적인 풍요로움을 더했습니다.\n",
            "*   **감각적인 표현:** \"따스한 햇살\", \"향긋한 꽃내음\", \"달콤한 속삭임\" 등 다양한 감각을 활용하여 사랑의 감정을 표현했습니다. 특히 \"마주 잡은 두 손\"이라는 촉각적 이미지를 추가하여 설렘과 따스함을 더했습니다.\n",
            "*   **기승전결 구조 고려:** 처음에는 벚꽃 아래 만남의 설렘을, 중간에는 따스한 감정을, 마지막에는 함께 걷는 미래를 약속하는 내용으로 구성하여 감정의 흐름을 자연스럽게 표현하고자 했습니다.\n",
            "*   **비유와 상징:** \"연분홍 꽃잎 흩날리듯\"이라는 비유를 사용하여 설렘과 시작의 느낌을 표현했습니다. 또한 \"봄날의 햇살처럼 따스한\"이라는 표현으로 사랑의 따뜻함을 상징적으로 표현했습니다.\n",
            "\n",
            "이전 시에 비해 봄의 이미지를 더 구체적으로 담아내고, 감각적인 표현을 풍부하게 사용하여 사랑의 설렘과 따뜻함을 더욱 효과적으로 전달하고자 노력했습니다. 다시 한번 피드백 주시면 감사하겠습니다.\n",
            "\n",
            "---------- critic ----------\n",
            "정말 훌륭하게 개선되었네요! 이전 시보다 훨씬 생생하고 감각적인 이미지가 눈에 띄네요. 특히, \"연분홍 꽃잎 흩날리듯, 어느새 스며든 그대 미소\"라는 첫 구절은 시각적인 아름다움과 함께 사랑의 시작을 알리는 듯한 설렘을 잘 표현했습니다. \"따스한 햇살 아래 마주 잡은, 수줍은 듯 붉게 물든 두 손\" 역시 촉각적인 이미지와 수줍은 감정이 잘 어우러져 독자의 공감을 불러일으킵니다.\n",
            "\n",
            "전반적으로 시의 흐름도 자연스럽고, 봄의 따스함과 사랑의 설렘이 조화롭게 어우러져 있습니다. 마지막 구절인 \"봄날의 햇살처럼 따스한, 영원히 함께 걸을 벚꽃길\"은 앞으로의 행복한 미래를 암시하며 여운을 남깁니다.\n",
            "\n",
            "몇 가지 사소한 의견을 덧붙이자면,\n",
            "\n",
            "*   **\"마주 잡은\" 대신 다른 표현 시도:** \"마주 잡은\"이라는 표현은 다소 평범하게 느껴질 수 있습니다. 예를 들어, \"포개어 잡은\", \"감싸 쥔\"과 같이 조금 더 특별한 표현을 사용해 보는 것도 좋을 것 같습니다.\n",
            "*   **시의 제목:** \"흩날리는 벚꽃 아래서\"라는 제목은 시의 내용을 잘 반영하고 있지만, 조금 더 함축적이거나 시적인 느낌을 주는 제목을 생각해 보는 것도 좋은 시도일 것 같습니다. 예를 들어, \"벚꽃 연가\", \"봄날의 약속\" 등과 같은 제목을 고려해 볼 수 있습니다.\n",
            "\n",
            "하지만, 현재 시의 완성도도 매우 높으며, 충분히 아름다운 작품입니다. 피드백을 적극적으로 반영하여 더욱 발전된 시를 쓰신 점이 인상적입니다. **EXCELLENT**\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TaskResult(messages=[TextMessage(source='user', models_usage=None, metadata={}, content='봄에 잘 어울리는 사랑에 관한 짧은 시를 써줘', type='TextMessage'), TextMessage(source='primary', models_usage=RequestUsage(prompt_tokens=25, completion_tokens=84), metadata={}, content='싱그러운 새싹 돋듯,\\n어느새 스며든 그대 향기.\\n\\n따스한 햇살 아래,\\n두 볼 붉게 물든 설렘.\\n\\n잔잔한 바람결처럼,\\n마음속 깊이 이는 사랑.\\n\\n봄날의 꿈결 같은,\\n영원히 함께할 우리의 이야기.\\n', type='TextMessage'), TextMessage(source='critic', models_usage=RequestUsage(prompt_tokens=148, completion_tokens=446), metadata={}, content=\"이 시는 봄의 싱그러움과 사랑의 설렘을 잘 표현했습니다. 몇 가지 건설적인 피드백을 드리자면 다음과 같습니다.\\n\\n*   **구체적인 이미지 사용:** 추상적인 표현 외에, 봄을 연상시키는 더욱 구체적인 이미지를 활용하면 독자의 감정을 더욱 자극할 수 있습니다. 예를 들어, '벚꽃 흩날리는 길', '개나리처럼 밝은 미소' 등 구체적인 이미지를 사용해 보세요.\\n*   **감각적인 표현:** 시각 외에 다른 감각(촉각, 후각, 청각)을 활용하여 사랑의 감정을 표현하면 더욱 풍부한 느낌을 줄 수 있습니다. 예를 들어, '따스한 손길', '달콤한 속삭임', '향긋한 꽃내음' 등을 활용해 보세요.\\n*   **기승전결 구조 고려:** 시에 기승전결 구조를 부여하여 감정의 흐름을 더욱 효과적으로 표현할 수 있습니다. 예를 들어, 처음에는 설렘을, 중간에는 사랑의 깊이를, 마지막에는 영원을 약속하는 내용을 담을 수 있습니다.\\n*   **단어의 반복 및 변주:** 특정 단어나 구절을 반복하거나 변주하여 시에 리듬감을 부여하고 강조하고 싶은 감정을 더욱 효과적으로 전달할 수 있습니다.\\n*   **비유와 상징:** 사랑을 비유하거나 상징하는 표현을 사용하여 시의 의미를 더욱 풍부하게 만들 수 있습니다. 예를 들어, 사랑을 '새싹'에 비유하거나, '햇살'로 상징할 수 있습니다.\\n\\n이러한 피드백을 바탕으로 시를 수정해 보시면 더욱 완성도 높은 작품을 만들 수 있을 것입니다.\", type='TextMessage'), TextMessage(source='primary', models_usage=RequestUsage(prompt_tokens=555, completion_tokens=524), metadata={}, content='피드백 감사합니다! 말씀해주신 부분을 반영하여 시를 수정해 보았습니다.\\n\\n***\\n\\n**흩날리는 벚꽃 아래서**\\n\\n연분홍 꽃잎 흩날리듯,\\n어느새 스며든 그대 미소.\\n\\n따스한 햇살 아래 마주 잡은,\\n수줍은 듯 붉게 물든 두 손.\\n\\n향긋한 꽃내음 실어오는,\\n달콤한 속삭임, 마음 깊이 스며.\\n\\n봄날의 햇살처럼 따스한,\\n영원히 함께 걸을 벚꽃길.\\n\\n***\\n\\n수정된 시에서는 다음과 같은 점들을 개선하고자 했습니다.\\n\\n*   **구체적인 이미지 사용:** \"흩날리는 벚꽃\", \"연분홍 꽃잎\", \"벚꽃길\" 등 봄을 연상시키는 구체적인 이미지를 사용하여 시각적인 풍요로움을 더했습니다.\\n*   **감각적인 표현:** \"따스한 햇살\", \"향긋한 꽃내음\", \"달콤한 속삭임\" 등 다양한 감각을 활용하여 사랑의 감정을 표현했습니다. 특히 \"마주 잡은 두 손\"이라는 촉각적 이미지를 추가하여 설렘과 따스함을 더했습니다.\\n*   **기승전결 구조 고려:** 처음에는 벚꽃 아래 만남의 설렘을, 중간에는 따스한 감정을, 마지막에는 함께 걷는 미래를 약속하는 내용으로 구성하여 감정의 흐름을 자연스럽게 표현하고자 했습니다.\\n*   **비유와 상징:** \"연분홍 꽃잎 흩날리듯\"이라는 비유를 사용하여 설렘과 시작의 느낌을 표현했습니다. 또한 \"봄날의 햇살처럼 따스한\"이라는 표현으로 사랑의 따뜻함을 상징적으로 표현했습니다.\\n\\n이전 시에 비해 봄의 이미지를 더 구체적으로 담아내고, 감각적인 표현을 풍부하게 사용하여 사랑의 설렘과 따뜻함을 더욱 효과적으로 전달하고자 노력했습니다. 다시 한번 피드백 주시면 감사하겠습니다.\\n', type='TextMessage'), TextMessage(source='critic', models_usage=RequestUsage(prompt_tokens=1118, completion_tokens=494), metadata={}, content='정말 훌륭하게 개선되었네요! 이전 시보다 훨씬 생생하고 감각적인 이미지가 눈에 띄네요. 특히, \"연분홍 꽃잎 흩날리듯, 어느새 스며든 그대 미소\"라는 첫 구절은 시각적인 아름다움과 함께 사랑의 시작을 알리는 듯한 설렘을 잘 표현했습니다. \"따스한 햇살 아래 마주 잡은, 수줍은 듯 붉게 물든 두 손\" 역시 촉각적인 이미지와 수줍은 감정이 잘 어우러져 독자의 공감을 불러일으킵니다.\\n\\n전반적으로 시의 흐름도 자연스럽고, 봄의 따스함과 사랑의 설렘이 조화롭게 어우러져 있습니다. 마지막 구절인 \"봄날의 햇살처럼 따스한, 영원히 함께 걸을 벚꽃길\"은 앞으로의 행복한 미래를 암시하며 여운을 남깁니다.\\n\\n몇 가지 사소한 의견을 덧붙이자면,\\n\\n*   **\"마주 잡은\" 대신 다른 표현 시도:** \"마주 잡은\"이라는 표현은 다소 평범하게 느껴질 수 있습니다. 예를 들어, \"포개어 잡은\", \"감싸 쥔\"과 같이 조금 더 특별한 표현을 사용해 보는 것도 좋을 것 같습니다.\\n*   **시의 제목:** \"흩날리는 벚꽃 아래서\"라는 제목은 시의 내용을 잘 반영하고 있지만, 조금 더 함축적이거나 시적인 느낌을 주는 제목을 생각해 보는 것도 좋은 시도일 것 같습니다. 예를 들어, \"벚꽃 연가\", \"봄날의 약속\" 등과 같은 제목을 고려해 볼 수 있습니다.\\n\\n하지만, 현재 시의 완성도도 매우 높으며, 충분히 아름다운 작품입니다. 피드백을 적극적으로 반영하여 더욱 발전된 시를 쓰신 점이 인상적입니다. **EXCELLENT**\\n', type='TextMessage')], stop_reason=\"Text 'EXCELLENT' mentioned\")"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "출력이 어떻게 되는지 확인하셨나요?  \n",
        "아래 같이 나오는 TaskResult의 경우, await Console(team.run_stream()) 이 반환하는 값을 받아주는 변수를 선언한다면 출력되지 않습니다.  "
      ],
      "metadata": {
        "id": "wLCML5HcnVJ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 6 : Human in the loop"
      ],
      "metadata": {
        "id": "6AENgJTxhq-f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "유저 또한 UserProxyAgent로 에이전트간의 대화에 참여할 수 있습니다!  \n"
      ],
      "metadata": {
        "id": "sHPdFXkRnlfb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from autogen_agentchat.agents import UserProxyAgent"
      ],
      "metadata": {
        "id": "8KI2VMDdhrfK"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_proxy = UserProxyAgent(\"user_proxy\", input_func=input)"
      ],
      "metadata": {
        "id": "U0e0wMjVhrh8"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "대화 종료를 위해 종료 조건을 설정해봅시다  \n",
        "위에 설정한대로, 대화를 마치기 위해서는 APPROVE를, 피드백을 주고 싶다면 원하는 명령을 입력하세요!"
      ],
      "metadata": {
        "id": "vD_7aMM6oGW7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a team with the primary and critic agents.\n",
        "team = RoundRobinGroupChat([poet_agent, user_proxy], termination_condition=text_termination)"
      ],
      "metadata": {
        "id": "XU40laSUhrnr"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stream = team.run_stream(task=\"봄에 잘 어울리는 사랑에 관한 짧은 시를 써줘\")"
      ],
      "metadata": {
        "id": "RLIK661XmAs_"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "await Console(stream)  # Stream the messages to the console."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQ4_Rzoml86E",
        "outputId": "edb6608b-2037-428f-84b3-73b15a21c444"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------- user ----------\n",
            "봄에 잘 어울리는 사랑에 관한 짧은 시를 써줘\n",
            "---------- primary ----------\n",
            "개나리 울타리 아래,\n",
            "수줍게 핀 첫 만남의 미소.\n",
            "싱그러운 바람처럼 스며든,\n",
            "그대 향기에 설레는 오후.\n",
            "\n",
            "톡톡 터지는 꽃망울처럼,\n",
            "점점 커져가는 풋풋한 마음.\n",
            "따스한 햇살 아래 나란히 걷는,\n",
            "봄날의 꿈결 같은 우리의 사랑.\n",
            "\n",
            "새싹처럼 돋아나는 믿음,\n",
            "영원히 함께할 약속의 씨앗.\n",
            "푸르른 잎새처럼 싱그러운,\n",
            "사랑으로 물든 우리의 봄날.\n",
            "\n",
            "Enter your response: 라임을 맞춰줘\n",
            "---------- user_proxy ----------\n",
            "라임을 맞춰줘\n",
            "---------- primary ----------\n",
            "개나리 담장 아래,\n",
            "수줍은 미소**에**.\n",
            "바람결 스미듯,\n",
            "향기에 설레**네**.\n",
            "\n",
            "꽃망울 터지듯,\n",
            "마음도 커**지네**.\n",
            "햇살 아래 걷는 길,\n",
            "꿈결 같**네**.\n",
            "\n",
            "새싹 돋듯 피어,\n",
            "약속의 씨앗**에**.\n",
            "잎새처럼 푸르게,\n",
            "사랑 물드**네**.\n",
            "\n",
            "Enter your response: EXCELLENT\n",
            "---------- user_proxy ----------\n",
            "EXCELLENT\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TaskResult(messages=[TextMessage(source='user', models_usage=None, metadata={}, content='봄에 잘 어울리는 사랑에 관한 짧은 시를 써줘', type='TextMessage'), TextMessage(source='primary', models_usage=RequestUsage(prompt_tokens=1097, completion_tokens=151), metadata={}, content='개나리 울타리 아래,\\n수줍게 핀 첫 만남의 미소.\\n싱그러운 바람처럼 스며든,\\n그대 향기에 설레는 오후.\\n\\n톡톡 터지는 꽃망울처럼,\\n점점 커져가는 풋풋한 마음.\\n따스한 햇살 아래 나란히 걷는,\\n봄날의 꿈결 같은 우리의 사랑.\\n\\n새싹처럼 돋아나는 믿음,\\n영원히 함께할 약속의 씨앗.\\n푸르른 잎새처럼 싱그러운,\\n사랑으로 물든 우리의 봄날.\\n', type='TextMessage'), UserInputRequestedEvent(source='user_proxy', models_usage=None, metadata={}, request_id='5b298f11-3bb6-4ab9-a3dc-3271dfef8a9f', content='', type='UserInputRequestedEvent'), TextMessage(source='user_proxy', models_usage=None, metadata={}, content='라임을 맞춰줘', type='TextMessage'), TextMessage(source='primary', models_usage=RequestUsage(prompt_tokens=1254, completion_tokens=105), metadata={}, content='개나리 담장 아래,\\n수줍은 미소**에**.\\n바람결 스미듯,\\n향기에 설레**네**.\\n\\n꽃망울 터지듯,\\n마음도 커**지네**.\\n햇살 아래 걷는 길,\\n꿈결 같**네**.\\n\\n새싹 돋듯 피어,\\n약속의 씨앗**에**.\\n잎새처럼 푸르게,\\n사랑 물드**네**.\\n', type='TextMessage'), UserInputRequestedEvent(source='user_proxy', models_usage=None, metadata={}, request_id='a5305f37-df4f-4096-9ccc-585b6088ac78', content='', type='UserInputRequestedEvent'), TextMessage(source='user_proxy', models_usage=None, metadata={}, content='EXCELLENT', type='TextMessage')], stop_reason=\"Text 'EXCELLENT' mentioned\")"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "수고하셨습니다!"
      ],
      "metadata": {
        "id": "zkwDTVhwepeT"
      }
    }
  ]
}